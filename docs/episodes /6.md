# 6. Batch correction and data set integration 

!!! quote ""

    Often, single-cell experiments are done by processing samples in multiple batches. This may be related to logistical constraints such as the inability to run all experimental conditions in parallel, or more extreme cases where samples are processed in different laboratories, by different people and even sequenced with different technologies (e.g. samples from human patients collected in different hospitals). These differences across sample batches very often result in global gene expression differences across those batches. Since batch-to-batch transcriptomic differences are likely unrelated to biological differences, we would ideally want “remove” them before drawing inferences about our cell populations.

    Biases due to batch effects are not new to single-cell RNA-seq. Indeed, several methods have been previously developed for standard bulk RNA-seq approaches. Some of those approaches rely on linear models that “regress out” the batch effect, assuming that the cell composition is similar across batches. However, in single-cell RNA-seq we may very often expect changes in cell compositions across batches (e.g. in our course data we have data from cancer samples such as ETV6-RUNX as well as a reference panel of healthy blood cells, PBMMCs). Therefore, methods are necessary that can deal with with heterogeneity across batches.

    In recent years, several methods have been developed to deal with this challenge ([too many to list here!](https://www.scrna-tools.org/tools?sort=name&cats=Integration)). Some of the most popular ones include the Mutual Nearest Neighbours (MNN) algorithm, a Principal Components Analysis-based clustering method implemented in the package HARMONY and a method that combines Canonical Correlation Analysis (CCC) and MNN implemented in the package Seurat 3. These methods have been shown to perform well in several benchmark studies (e.g. Luecken et al 2022 and Tran et al 2020), although one important message from these studies is that no **single method is universally the best in all situations**. For example, some methods may be better at preserving small populations of cells as separate groups in the integrated data at the cost of poorer overall integration, while others may be better at removing batch effects at the cost of also removing some biological signal.

    In this section we will apply the **Mutual Nearest Neighbours (MNN)** algorithm, which is readily available to use with the `SingleCellExperiment`` object we’ve been working with so far. However, other methods can be applied to the data in a similar manner (each package may use a slightly different syntax, but they mostly start with either a matrix of counts or a PCA projection). Therefore, what we will explore in this section - visualisation of the integrated data, looking at mixture of cell populations, etc. - can be done with those other methods as well.

## Example data set - PBMMC_1 technical replicates

To demonstrate the integration process, we will two samples from the Caron dataset that will illustrate the purposes of dataset integration with batch correction. One is the PBMMC_1 sample that we have already seen, the other is a technical replicate derived from the same sample material (we will use our previous SCE object in a later exercise).

Whilst the two samples come from distinct 10X runs they are derived from the same starting material and therefore, if there was no batch effect, they should be identical. These samples have been processed as discussed up until this point in the course:

- Raw counts were imported from the cellranger output folder (using `DropletUtils::read10xCounts()`).
- Basic quality filtering was performed in each batch to remove cells that were outliers for total counts, number of detected genes and high percentage of mitochondrial counts (using `scuttle::quickPerCellQC()`).
- Reads were log-normalised using the deconvolution method (using `scuttle::computePooledFactors()`).

We already have the necessary objects prepared, and load them for this session:

!!! r-project "code"

    ```r
    library(scater)
    library(scran)
    library(batchelor)
    library(bluster)
    library(pheatmap)
    library(magrittr)
    ```
    ```r
    sce_rep1 <- readRDS("R_objects/PBMMC_1a_dimRed.rds")
    sce_rep2 <- readRDS("R_objects/PBMMC_1b_dimRed.rds")
    ```

    - First we should add information about which technical replicate each sample is. This is added as a new column in the colData DataFrame of the object.

    ```r
    colData(sce_rep1)$batch <- "1"
    colData(sce_rep2)$batch <- "2"
    ```

## Data Preparation

!!! circle-info ""

Before the data integration step, we need to prepare our data (we will later see how we can run all these steps with a single function, but it is good to see all the steps individually).

1. First we need to fit a mean-variance mode to each data set separately (using `scran::modelGeneVar()``). This will be used later to identify highly-variable genes (HVGs) in each batch.
2. Subset our objects to only include the set of genes that are common in both samples (in case different genes were filtered out).
3. Rescale the batches to account for different sequencing depths. We had previously log-normalised the counts in each batch. However, this did not take into account differences in total sequencing depth across different batches. This step therefore helps to bring the different batches to a “similar scale”, which helps with the data integration step.
4.Select variable genes (feature selection), by averaging the variance previously estimated in each batch separately. This will gives us genes that are highly variable across both batches.

!!! r-project-2 "Fit mean-variance mode to each data set"

    ```r
    gene_var_rep1 <- modelGeneVar(sce_rep1)
    gene_var_rep2 <- modelGeneVar(sce_rep2)
    ```